{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoT Training Optimization Benchmark\n",
    "\n",
    "This notebook tests different optimization strategies for the Mixture of Thoughts model.\n",
    "\n",
    "**Goal:** Identify bottlenecks and optimize training speed to match nanoGPT baseline (~11.79 ms/iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from mot.core.model import MixtureOfThoughtsTransformer, MoTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"torch.compile available: {hasattr(torch, 'compile')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data(seq_length=128, vocab_size=100, num_samples=100):\n",
    "    \"\"\"Create dummy data for testing\"\"\"\n",
    "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    return data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create test data\n",
    "data = create_dummy_data(seq_length=128, vocab_size=100, num_samples=100)\n",
    "print(f\"Test data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = MoTConfig(\n",
    "    vocab_size=100,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_thoughts=8,\n",
    "    max_position_embeddings=128\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Vocab size: {config.vocab_size}\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Thoughts: {config.num_thoughts}\")\n",
    "print(f\"  Sequence length: {config.max_position_embeddings}\")\n",
    "\n",
    "# Create model to check size\n",
    "test_model = MixtureOfThoughtsTransformer(config)\n",
    "params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"\\nModel parameters: {params:,}\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_training(model, data, device, num_iters=10, use_amp=False, desc=\"\"):\n",
    "    \"\"\"Benchmark training step speed\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {desc}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    times = []\n",
    "    \n",
    "    # Gradient scaler for AMP\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == 'cuda' else None\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Warming up...\")\n",
    "    for i in range(5):\n",
    "        batch = data[i:i+1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch)\n",
    "                loss = outputs['logits'].mean()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(batch)\n",
    "            loss = outputs['logits'].mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Actual benchmark\n",
    "    print(f\"Running {num_iters} iterations...\")\n",
    "    for i in range(num_iters):\n",
    "        batch = data[i:i+1].to(device)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch)\n",
    "                loss = outputs['logits'].mean()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(batch)\n",
    "            loss = outputs['logits'].mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Mean: {mean_time:.2f} ms\")\n",
    "    print(f\"  Std:  {std_time:.2f} ms\")\n",
    "    print(f\"  Min:  {np.min(times):.2f} ms\")\n",
    "    print(f\"  Max:  {np.max(times):.2f} ms\")\n",
    "    \n",
    "    return mean_time, std_time, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Baseline (No Optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = MixtureOfThoughtsTransformer(config).to(device)\n",
    "\n",
    "baseline_mean, baseline_std, baseline_times = benchmark_training(\n",
    "    model_baseline, \n",
    "    data, \n",
    "    device, \n",
    "    num_iters=10, \n",
    "    use_amp=False,\n",
    "    desc=\"Baseline (no optimizations)\"\n",
    ")\n",
    "\n",
    "del model_baseline\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: With AMP (Automatic Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_amp = MixtureOfThoughtsTransformer(config).to(device)\n",
    "\n",
    "amp_mean, amp_std, amp_times = benchmark_training(\n",
    "    model_amp,\n",
    "    data,\n",
    "    device,\n",
    "    num_iters=10,\n",
    "    use_amp=True,\n",
    "    desc=\"With AMP (FP16 mixed precision)\"\n",
    ")\n",
    "\n",
    "speedup_amp = baseline_mean / amp_mean\n",
    "print(f\"\\nSpeedup vs baseline: {speedup_amp:.2f}x\")\n",
    "\n",
    "del model_amp\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: With torch.compile + AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(torch, 'compile') and device.type == 'cuda':\n",
    "    print(\"Compiling model... (this may take 1-2 minutes on first run)\")\n",
    "    \n",
    "    model_compiled = MixtureOfThoughtsTransformer(config).to(device)\n",
    "    model_compiled = torch.compile(model_compiled)\n",
    "    \n",
    "    compiled_mean, compiled_std, compiled_times = benchmark_training(\n",
    "        model_compiled,\n",
    "        data,\n",
    "        device,\n",
    "        num_iters=10,\n",
    "        use_amp=True,\n",
    "        desc=\"With torch.compile + AMP\"\n",
    "    )\n",
    "    \n",
    "    speedup_compiled = baseline_mean / compiled_mean\n",
    "    print(f\"\\nSpeedup vs baseline: {speedup_compiled:.2f}x\")\n",
    "    \n",
    "    del model_compiled\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"torch.compile not available or not on CUDA\")\n",
    "    compiled_mean, compiled_std, compiled_times = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "methods = ['Baseline', 'AMP']\n",
    "means = [baseline_mean, amp_mean]\n",
    "stds = [baseline_std, amp_std]\n",
    "\n",
    "if compiled_mean is not None:\n",
    "    methods.append('Compile + AMP')\n",
    "    means.append(compiled_mean)\n",
    "    stds.append(compiled_std)\n",
    "\n",
    "# Create bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Mean times\n",
    "colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "bars = ax1.bar(methods, means, yerr=stds, capsize=5, color=colors[:len(methods)])\n",
    "ax1.set_ylabel('Time per iteration (ms)', fontsize=12)\n",
    "ax1.set_title('Training Speed Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, means):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mean:.2f} ms',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Speedup\n",
    "speedups = [1.0]  # Baseline\n",
    "speedups.append(baseline_mean / amp_mean)\n",
    "if compiled_mean is not None:\n",
    "    speedups.append(baseline_mean / compiled_mean)\n",
    "\n",
    "bars2 = ax2.bar(methods, speedups, color=colors[:len(methods)])\n",
    "ax2.set_ylabel('Speedup (vs baseline)', fontsize=12)\n",
    "ax2.set_title('Relative Performance', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, speedup in zip(bars2, speedups):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{speedup:.2f}x',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChart saved as 'optimization_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with nanoGPT Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanoGPT baseline from testing\n",
    "nanogpt_time = 11.79  # ms/iter\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Comparison with nanoGPT Baseline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nnanoGPT (baseline):     {nanogpt_time:.2f} ms/iter\")\n",
    "print(f\"MoT Baseline:           {baseline_mean:.2f} ms/iter ({baseline_mean/nanogpt_time:.1f}x slower)\")\n",
    "print(f\"MoT with AMP:           {amp_mean:.2f} ms/iter ({amp_mean/nanogpt_time:.1f}x slower)\")\n",
    "\n",
    "if compiled_mean is not None:\n",
    "    print(f\"MoT with Compile+AMP:   {compiled_mean:.2f} ms/iter ({compiled_mean/nanogpt_time:.1f}x slower)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Insights\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Model computation optimizations (AMP, compile) provide speedup\")\n",
    "print(\"2. But still much slower than nanoGPT baseline\")\n",
    "print(\"3. Remaining bottleneck: DataLoader overhead in full training loop\")\n",
    "print(\"\\nSolution: Adopt nanoGPT's data loading approach:\")\n",
    "print(\"  - Use numpy memmap for direct data access\")\n",
    "print(\"  - Eliminate DataLoader multiprocessing overhead\")\n",
    "print(\"  - Simplify training loop to minimize Python overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: {params:,} parameters\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Baseline:           {baseline_mean:7.2f} ms  (1.00x)\")\n",
    "print(f\"  + AMP:              {amp_mean:7.2f} ms  ({baseline_mean/amp_mean:.2f}x speedup)\")\n",
    "if compiled_mean is not None:\n",
    "    print(f\"  + Compile + AMP:    {compiled_mean:7.2f} ms  ({baseline_mean/compiled_mean:.2f}x speedup)\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  ✅ Enable AMP for ~{:.0f}% speedup\".format((1 - amp_mean/baseline_mean) * 100))\n",
    "if compiled_mean is not None:\n",
    "    print(\"  ✅ Enable torch.compile for additional ~{:.0f}% speedup\".format((1 - compiled_mean/amp_mean) * 100))\n",
    "print(\"  ⚠️  Set num_workers=0 to eliminate multiprocessing overhead\")\n",
    "print(\"  🎯 Ultimate goal: Match nanoGPT's {:.2f} ms/iter\".format(nanogpt_time))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on these results:\n",
    "\n",
    "1. **Immediate fixes** (already applied):\n",
    "   - Set `num_workers: 0` in config\n",
    "   - Set `use_compile: true` in config\n",
    "\n",
    "2. **Test full training**:\n",
    "   ```bash\n",
    "   python scripts/train_with_config.py configs/training/small.yaml --batch-size 128\n",
    "   ```\n",
    "\n",
    "3. **If still slow, adopt nanoGPT's approach**:\n",
    "   - Replace DataLoader with numpy memmap\n",
    "   - Simplify training loop\n",
    "   - Minimize Python overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
